"""High-level orchestration for running LLM-based tender analysis jobs."""

from __future__ import annotations

import logging
import os
import tempfile
import time
import uuid
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, Iterable, Optional

from ..analyzer.tender_llm import TenderLLMAnalyzer
from ..extractors.dispatcher import extract_text_from_file
from ..storage import AnalysisJobRecord, InMemoryJobStore

logger = logging.getLogger(__name__)


@dataclass
class JobPayload:
    """User-provided context for an analysis job."""

    text: Optional[str] = None
    filename: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class AnalysisService:
    """Submit and execute analysis jobs with pluggable storage."""

    def __init__(
        self,
        analyzer: TenderLLMAnalyzer,
        store: Optional[InMemoryJobStore] = None,
        observers: Optional[Iterable[Callable[[AnalysisJobRecord], None]]] = None,
    ) -> None:
        self.analyzer = analyzer
        self.store = store or InMemoryJobStore()
        self._observers = list(observers or [])

    # ------------------------------------------------------------------ API
    def create_job(
        self,
        source: str,
        *,
        filename: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> AnalysisJobRecord:
        job = AnalysisJobRecord(
            job_id=str(uuid.uuid4()),
            status="pending",
            source=source,
            filename=filename,
            metadata=metadata or {},
            created_at=time.time(),
        )
        created = self.store.create(job)
        self._notify(created.job_id)
        return created

    def process_text(
        self,
        job_id: str,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> AnalysisJobRecord:
        job = self.store.get(job_id)
        if not job:
            raise KeyError(f"Job {job_id} not found")
        metadata = metadata or {}
        combined_metadata = dict(job.metadata)
        combined_metadata.update(metadata)
        combined_metadata.pop("preprocess", None)
        self.store.update(
            job_id,
            status="processing",
            started_at=time.time(),
            text_length=len(text),
            metadata=combined_metadata,
            source_text=text,
        )
        self._notify(job_id)
        try:
            result = self.analyzer.analyze(text)
            combined_metadata.update(result.pop("metadata", {}))
            self.store.update(job_id, metadata=combined_metadata)
            self.store.update(job_id, status="completed", result=result, completed_at=time.time())
            self._notify(job_id)
        except Exception as exc:  # pragma: no cover - defensive
            self.store.update(job_id, status="failed", error=str(exc), completed_at=time.time())
            self._notify(job_id)
            raise
        finally:
            job = self.store.get(job_id)
            if job:
                self._notify(job.job_id)
        assert job is not None
        return job

    def process_file_upload(
        self,
        job_id: str,
        path: str,
        filename: Optional[str] = None,
        content_type: Optional[str] = None,
    ) -> AnalysisJobRecord:
        logger.info(
            "process_file_upload start job=%s filename=%s content_type=%s path=%s",
            job_id,
            filename or "",
            content_type or "",
            path,
        )
        text, meta = extract_text_from_file(path, filename=filename, content_type=content_type)
        metadata = meta or {}
        if not text:
            debug_bits = []
            detected = metadata.get("detected_type")
            if detected:
                debug_bits.append(f"检测类型：{detected}")
            size_hint = metadata.get("byte_size")
            if size_hint:
                debug_bits.append(f"文件大小：{size_hint} 字节")
            if metadata.get("fallback"):
                debug_bits.append(f"fallback：{metadata['fallback']}")
            if metadata.get("ocr_used"):
                debug_bits.append("OCR 已尝试")
            extra = f"（{'，'.join(debug_bits)}）" if debug_bits else ""
            error_message = f"未能从文件中提取文本或文本为空{extra}"
            logger.error(
                "process_file_upload failed job=%s filename=%s meta=%s",
                job_id,
                filename or "",
                metadata,
            )
            self.store.update(
                job_id,
                status="failed",
                error=error_message,
                metadata=metadata,
                completed_at=time.time(),
            )
            job = self.store.get(job_id)
            assert job is not None
            self._notify(job.job_id)
            return job
        metadata["extracted_text_length"] = str(len(text))
        logger.info(
            "process_file_upload success job=%s filename=%s length=%s meta=%s",
            job_id,
            filename or "",
            metadata["extracted_text_length"],
            metadata,
        )
        if filename and "filename" not in metadata:
            metadata["filename"] = filename
        return self.process_text(job_id, text, metadata=metadata)

    # ---------------------------------------------------------------- Helpers
    def submit_text(
        self,
        *,
        text: str,
        filename: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        async_runner: Optional[Any] = None,
    ) -> AnalysisJobRecord:
        job = self.create_job(source="text", filename=filename, metadata=metadata)
        if async_runner:
            async_runner(self.process_text, job.job_id, text, metadata)
            stored = self.store.get(job.job_id)
            assert stored is not None
            return stored
        return self.process_text(job.job_id, text, metadata)

    def submit_file(
        self,
        file_obj,
        *,
        filename: Optional[str] = None,
        content_type: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        async_runner: Optional[Any] = None,
    ) -> AnalysisJobRecord:
        job = self.create_job(source="file", filename=filename, metadata=metadata)
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(file_obj.read())
            tmp_path = tmp.name
        if async_runner:
            async_runner(self._process_file_async, job.job_id, tmp_path, filename, content_type)
            stored = self.store.get(job.job_id)
            assert stored is not None
            return stored
        try:
            return self.process_file_upload(job.job_id, tmp_path, filename, content_type)
        finally:
            self._safe_unlink(tmp_path)

    # ---------------------------------------------------------------- Public read
    def serialize_job(self, job_id: str, *, include_result: bool = True) -> Dict[str, Any]:
        job = self.store.get(job_id)
        if not job:
            raise KeyError(job_id)
        data = self._serialize_job_record(job, include_result=include_result)
        return data

    def list_jobs(self) -> Dict[str, Any]:
        jobs_iterable = self.store.list()
        jobs = [
            self._serialize_job_record(j, include_result=False)
            for j in sorted(jobs_iterable, key=lambda x: x.created_at, reverse=True)
        ]
        return {"jobs": jobs}

    def delete_job(self, job_id: str) -> bool:
        return self.store.delete(job_id)

    # ---------------------------------------------------------------- Internal
    def add_observer(self, callback: Callable[[AnalysisJobRecord], None]) -> None:
        self._observers.append(callback)

    def _notify(self, job_id: str) -> None:
        if not self._observers:
            return
        job = self.store.get(job_id)
        if not job:
            return
        for observer in list(self._observers):
            try:
                observer(job)
            except Exception:  # pragma: no cover - defensive logging
                logger.exception("Job observer failed for job %s", job_id)

    @staticmethod
    def _serialize_job_record(job: AnalysisJobRecord, include_result: bool = True) -> Dict[str, Any]:
        payload = {
            "job_id": job.job_id,
            "status": job.status,
            "source": job.source,
            "filename": job.filename,
            "text_length": job.text_length,
            "created_at": job.created_at,
            "started_at": job.started_at,
            "completed_at": job.completed_at,
            "metadata": job.metadata,
            "error": job.error,
            "has_source_text": bool(job.source_text),
        }
        if include_result and job.result is not None:
            payload["result"] = job.result
        return payload

    def get_source_snippet(
        self,
        job_id: str,
        start: int,
        end: Optional[int] = None,
        window: int = 120,
    ) -> Dict[str, Any]:
        job = self.store.get(job_id)
        if not job or job.source_text is None:
            raise KeyError(job_id)
        text = job.source_text
        length = len(text)
        if length == 0:
            return {"job_id": job_id, "start": 0, "end": 0, "excerpt": "", "context": "", "length": 0}

        start = max(0, min(length, int(start)))
        if end is None:
            end = start + 1
        end = max(start, min(length, int(end)))
        window = max(0, int(window))
        excerpt = text[start:end]
        context_start = max(0, start - window)
        context_end = min(length, end + window)
        context = text[context_start:context_end]
        return {
            "job_id": job_id,
            "start": start,
            "end": end,
            "excerpt": excerpt,
            "context": context,
            "context_start": context_start,
            "context_end": context_end,
            "length": length,
        }

    # ---------------------------------------------------------------- Internals
    def _process_file_async(
        self,
        job_id: str,
        path: str,
        filename: Optional[str],
        content_type: Optional[str],
    ) -> None:
        try:
            self.process_file_upload(job_id, path, filename, content_type)
        finally:
            self._safe_unlink(path)

    @staticmethod
    def _safe_unlink(path: str) -> None:
        try:
            os.unlink(path)
        except FileNotFoundError:
            pass


def background_runner(background_tasks, func, *args, **kwargs):
    """Helper that forwards execution to FastAPI BackgroundTasks if provided."""

    if background_tasks is None:
        raise RuntimeError("Background runner requires FastAPI BackgroundTasks instance")
    background_tasks.add_task(func, *args, **kwargs)
